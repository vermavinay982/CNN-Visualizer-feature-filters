{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "defined-intervention",
   "metadata": {},
   "source": [
    "input takes 224,224 3 channel image\n",
    "64 channels after conv1\n",
    "64 again with same dimesion conv2\n",
    "\n",
    "maxpooling reduced image to half - 112,112 64 channels\n",
    "128 channels conv 3\n",
    "128 channels conv 4\n",
    "\n",
    "maxpool again to make half 56,56 128 channels\n",
    "256 channel conv 5\n",
    "256 channel conv 6\n",
    "256 channel conv 7\n",
    "\n",
    "maxpool again to reduce 28,28 with 256 channels\n",
    "512 channel conv 8\n",
    "512 channel conv 9\n",
    "512 channel conv 10\n",
    "\n",
    "maxpool again to make 14,14 with 512 channels\n",
    "512 channel conv 11\n",
    "512 channel conv 12\n",
    "512 channel conv 13\n",
    "\n",
    "maxpool to make 7,7 with 512 channels\n",
    "flatten to make 25088 values\n",
    "fully connected 1 4096\n",
    "fully connected 2 4096\n",
    "\n",
    "final dense prediction with 1000 outputs of imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alpine-checkout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load vgg model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "model = VGG16()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cosmetic-modem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block1_pool\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block2_pool\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_pool\n",
      "flatten\n",
      "fc1\n",
      "fc2\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "for layers in model.layers:\n",
    "    print(layers.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adopted-announcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block1_conv1 (3, 3, 3, 64)\n",
      "block1_conv2 (3, 3, 64, 64)\n",
      "block2_conv1 (3, 3, 64, 128)\n",
      "block2_conv2 (3, 3, 128, 128)\n",
      "block3_conv1 (3, 3, 128, 256)\n",
      "block3_conv2 (3, 3, 256, 256)\n",
      "block3_conv3 (3, 3, 256, 256)\n",
      "block4_conv1 (3, 3, 256, 512)\n",
      "block4_conv2 (3, 3, 512, 512)\n",
      "block4_conv3 (3, 3, 512, 512)\n",
      "block5_conv1 (3, 3, 512, 512)\n",
      "block5_conv2 (3, 3, 512, 512)\n",
      "block5_conv3 (3, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-blues",
   "metadata": {},
   "source": [
    "channel last format\n",
    "depth of the filter must match the depth of input for the filter\n",
    "\n",
    "for an input image with 3 channel - RGB \n",
    "we can visualise one filter as a plot with 3 images, one for each channel\n",
    "\n",
    "or can compress 3 channels into 1 - still 63 more to visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "positive-nepal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3, 64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for the first layer\n",
    "# filter, biases = model.layers[0].output.get_weights() # tensor has not object get_weights\n",
    "# output of the layer is a tensor - that is not weights\n",
    "\n",
    "type(model.layers[1]) # tensorflow.python.keras.layers.convolutional.Conv2D\n",
    "\n",
    "filter, biases = model.layers[1].get_weights() # tensor has not object get_weights\n",
    "filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "apart-introduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_3:0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].output.shape\n",
    "model.layers[0].output.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "harmful-stand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_3:0'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].input.name\n",
    "\n",
    "# the input of one is the output of previous one\n",
    "# this is just like linked lists - covering all the values of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "retired-yukon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# plotting the filters\n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:,:,:,i]\n",
    "    # plot the filter\n",
    "    for j in range(3):\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # plot filter channels in grayscale\n",
    "        plt.imshow(f[:,:,j], cmap='gray')\n",
    "        ix += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "animal-playlist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.00015223,  0.00135788, -0.00504664, ...,  0.00349678,\n",
       "          0.00127733, -0.01760248],\n",
       "        [-0.0042193 , -0.00428369, -0.00488854, ..., -0.00082621,\n",
       "          0.00947703, -0.01716787],\n",
       "        [ 0.00142437, -0.00419647, -0.00811489, ...,  0.00114329,\n",
       "          0.01449443, -0.01928296]],\n",
       "\n",
       "       [[ 0.00873817, -0.00740686,  0.00207562, ..., -0.00034642,\n",
       "          0.00483277, -0.0131816 ],\n",
       "        [ 0.00648991, -0.00646745,  0.00433064, ..., -0.0050802 ,\n",
       "          0.01188056, -0.00886193],\n",
       "        [ 0.00896835, -0.00575299,  0.00268265, ..., -0.00881005,\n",
       "          0.01205773, -0.01843777]],\n",
       "\n",
       "       [[ 0.01878544, -0.01281616,  0.00274524, ..., -0.00472345,\n",
       "          0.00255884, -0.00889258],\n",
       "        [ 0.02188619, -0.01036999,  0.00381488, ..., -0.00954801,\n",
       "          0.00895035, -0.0034732 ],\n",
       "        [ 0.02062797, -0.00699785,  0.00196754, ..., -0.00671082,\n",
       "          0.01000218, -0.01846195]]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "composite-creek",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:  -0.09288482 0.28699666\n",
      "after:  0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# now lets normalize the filters\n",
    "# using min max normalization - that is very easy\n",
    "# sub from the min, divide by the range ( max - min )\n",
    "\n",
    "# initially we do number/max and get result - but now lets subract min too\n",
    "\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "norm_filters = (filters - f_min)/(f_max - f_min)\n",
    "print('before: ',f_min, f_max)\n",
    "f_min, f_max = norm_filters.min(), norm_filters.max()\n",
    "print('after: ',f_min, f_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "paperback-sympathy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize after normalization \n",
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    f = norm_filters[:,:,:,i]\n",
    "    for j in range(3):\n",
    "        ax = plt.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        plt.imshow(f[:,:,j], cmap='gray')\n",
    "        ix += 1\n",
    "plt.show()\n",
    "\n",
    "# 6 out of 64 filters in first conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "involved-opposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 512)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bizarre-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will not be able to see the 2nd conv layer filters\n",
    "# and so futher more - 64 channels in a row for 64 filters (64x64 subplots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-unknown",
   "metadata": {},
   "source": [
    "## Visualizing Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "younger-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 block1_conv1 (None, 224, 224, 64) (3, 3, 3, 64)\n",
      "2 2 block1_conv2 (None, 224, 224, 64) (3, 3, 64, 64)\n",
      "4 3 block2_conv1 (None, 112, 112, 128) (3, 3, 64, 128)\n",
      "5 4 block2_conv2 (None, 112, 112, 128) (3, 3, 128, 128)\n",
      "7 5 block3_conv1 (None, 56, 56, 256) (3, 3, 128, 256)\n",
      "8 6 block3_conv2 (None, 56, 56, 256) (3, 3, 256, 256)\n",
      "9 7 block3_conv3 (None, 56, 56, 256) (3, 3, 256, 256)\n",
      "11 8 block4_conv1 (None, 28, 28, 512) (3, 3, 256, 512)\n",
      "12 9 block4_conv2 (None, 28, 28, 512) (3, 3, 512, 512)\n",
      "13 10 block4_conv3 (None, 28, 28, 512) (3, 3, 512, 512)\n",
      "15 11 block5_conv1 (None, 14, 14, 512) (3, 3, 512, 512)\n",
      "16 12 block5_conv2 (None, 14, 14, 512) (3, 3, 512, 512)\n",
      "17 13 block5_conv3 (None, 14, 14, 512) (3, 3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "# activation maps - called feature maps capture the result of applying filter to input\n",
    "# input image or another feature map\n",
    "\n",
    "# idea is to visualize feature map for specific input image and understand what\n",
    "# features of input are detected or preserved in feature maps.\n",
    "\n",
    "# maps close to input - detect find grained details\n",
    "# maps close to output - capture general features\n",
    "\n",
    "# sumamrize feature map size for each conv layer\n",
    "conv_count = 0\n",
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    conv_count += 1\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(i, conv_count, layer.name, layer.output.shape, filters.shape)\n",
    "    \n",
    "# in total there are 16 layers - 13 conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "exact-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a new model that is subset of vgg16 model\n",
    "# model would have same input layer as original model\n",
    "# output will be output of given conv layer - which would be the activation of the layer or feature map\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "model2 = Model(inputs=model.inputs, outputs=model.layers[1].output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "incomplete-southwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "=================================================================\n",
      "Total params: 1,792\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-highlight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
